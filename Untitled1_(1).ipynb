{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5xb-dQuViQM",
        "outputId": "85d0a046-6895-456a-b375-3a932accda44"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import defaultdict\n",
        "from faker import Faker\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, List\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class BusinessDataset(Dataset):\n",
        "    def __init__(self, X, y_revenue, y_risk, y_profitability, y_churn, y_suggestions, month_nums, quarter_nums):\n",
        "        self.X = X if torch.is_tensor(X) else torch.FloatTensor(X)\n",
        "        self.y_revenue = y_revenue if torch.is_tensor(y_revenue) else torch.FloatTensor(y_revenue)\n",
        "        self.y_risk = y_risk if torch.is_tensor(y_risk) else torch.LongTensor(y_risk)\n",
        "        self.y_profitability = y_profitability if torch.is_tensor(y_profitability) else torch.FloatTensor(y_profitability)\n",
        "        self.y_churn = y_churn if torch.is_tensor(y_churn) else torch.FloatTensor(y_churn)\n",
        "        self.y_suggestions = y_suggestions if torch.is_tensor(y_suggestions) else torch.LongTensor(y_suggestions)\n",
        "        self.month_nums = month_nums if torch.is_tensor(month_nums) else torch.LongTensor(month_nums)\n",
        "        self.quarter_nums = quarter_nums if torch.is_tensor(quarter_nums) else torch.LongTensor(quarter_nums)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.X[idx],\n",
        "            self.y_revenue[idx],\n",
        "            self.y_risk[idx],\n",
        "            self.y_profitability[idx],\n",
        "            self.y_churn[idx],\n",
        "            self.y_suggestions[idx],\n",
        "            self.month_nums[idx],\n",
        "            self.quarter_nums[idx]\n",
        "        )\n",
        "\n",
        "\n",
        "class EnhancedBusinessPredictor(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 128, num_risk_classes: int = 3, num_suggestion_classes: int = 6):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Temporal embedding layers\n",
        "        self.month_embedding = nn.Embedding(13, 16)  # 12 months + padding\n",
        "        self.quarter_embedding = nn.Embedding(5, 8)   # 4 quarters + padding\n",
        "\n",
        "        # Linear layer to match dimensions before LSTM\n",
        "        self.input_proj = nn.Linear(input_dim + 24, hidden_dim)\n",
        "\n",
        "        # LSTM for temporal dependencies\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=2,\n",
        "            dropout=0.2,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Task-specific layers with dropout\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # Revenue prediction\n",
        "        self.revenue_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Risk assessment\n",
        "        self.risk_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, num_risk_classes)\n",
        "        )\n",
        "\n",
        "        # Profitability prediction\n",
        "        self.profitability_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Churn prediction\n",
        "        self.churn_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Action recommendation\n",
        "        self.suggestion_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_suggestion_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, months, quarters):\n",
        "        # Get temporal embeddings\n",
        "        month_emb = self.month_embedding(months)\n",
        "        quarter_emb = self.quarter_embedding(quarters)\n",
        "        temporal_features = torch.cat([month_emb, quarter_emb], dim=-1)\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat([x, temporal_features], dim=-1)\n",
        "\n",
        "        # Project to correct dimension\n",
        "        projected = self.input_proj(combined)\n",
        "        projected = projected.unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        # Process through LSTM\n",
        "        lstm_out, _ = self.lstm(projected)\n",
        "\n",
        "        # Get final hidden state\n",
        "        features = lstm_out.squeeze(1)\n",
        "        features = self.dropout(features)\n",
        "\n",
        "        # Task-specific predictions\n",
        "        return {\n",
        "            'revenue': self.revenue_layer(features),\n",
        "            'risk': self.risk_layer(features),\n",
        "            'profitability': self.profitability_layer(features),\n",
        "            'churn': self.churn_layer(features),\n",
        "            'suggestions': self.suggestion_layer(features)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Linear(in_channels, out_channels)\n",
        "        self.conv2 = nn.Linear(out_channels, out_channels)\n",
        "        self.norm1 = nn.LayerNorm(out_channels)\n",
        "        self.norm2 = nn.LayerNorm(out_channels)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Projection shortcut if dimensions don't match\n",
        "        self.shortcut = nn.Linear(in_channels, out_channels) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = F.gelu(x)  # GELU activation for better gradient flow\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x += identity\n",
        "        return F.gelu(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add sequence length dimension\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "        x = self.norm(x)\n",
        "        return x.squeeze(1)\n",
        "\n",
        "class TaskNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dims=[64, 32], dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def generate_business_data(n_samples):\n",
        "    np.random.seed(42)\n",
        "    fake = Faker()\n",
        "\n",
        "    def generate_pl_statement():\n",
        "        revenue = np.random.randint(500000, 5000000)\n",
        "        cogs = np.random.randint(100000, revenue)\n",
        "        gross_profit = revenue - cogs\n",
        "        operating_expenses = np.random.randint(100000, 1000000)\n",
        "        operating_income = gross_profit - operating_expenses\n",
        "        other_income = np.random.randint(10000, 100000)\n",
        "        net_profit = operating_income + other_income\n",
        "        return f\"Revenue: {revenue}\\nCOGS: {cogs}\\nGross Profit: {gross_profit}\\n\" \\\n",
        "               f\"Operating Expenses: {operating_expenses}\\nOperating Income: {operating_income}\\n\" \\\n",
        "               f\"Other Income: {other_income}\\nNet Profit: {net_profit}\"\n",
        "\n",
        "    def generate_balance_sheet():\n",
        "        current_assets = np.random.randint(500000, 2000000)\n",
        "        fixed_assets = np.random.randint(1000000, 5000000)\n",
        "        total_assets = current_assets + fixed_assets\n",
        "        current_liabilities = np.random.randint(100000, 500000)\n",
        "        long_term_liabilities = np.random.randint(200000, 1000000)\n",
        "        total_liabilities = current_liabilities + long_term_liabilities\n",
        "        equity = total_assets - total_liabilities\n",
        "        return f\"Current Assets: {current_assets}\\nFixed Assets: {fixed_assets}\\n\" \\\n",
        "               f\"Total Assets: {total_assets}\\nCurrent Liabilities: {current_liabilities}\\n\" \\\n",
        "               f\"Long-term Liabilities: {long_term_liabilities}\\nTotal Liabilities: {total_liabilities}\\n\" \\\n",
        "               f\"Equity: {equity}\"\n",
        "\n",
        "    def generate_recommendation_actions():\n",
        "        actions = [\n",
        "            \"Expand market reach\",\n",
        "            \"Optimize costs\",\n",
        "            \"Invest in marketing\",\n",
        "            \"Diversify products\",\n",
        "            \"Improve customer service\",\n",
        "            \"Increase R&D\",\n",
        "            \"Enhance loyalty programs\",\n",
        "            \"Cut overhead costs\"\n",
        "        ]\n",
        "        return random.choice(actions)\n",
        "\n",
        "    data = {\n",
        "        'P&L_Statement': [generate_pl_statement() for _ in range(n_samples)],\n",
        "        'Balance_Sheet': [generate_balance_sheet() for _ in range(n_samples)],\n",
        "        'Monthly_Revenue_Turnover': [fake.month_name() for _ in range(n_samples)],\n",
        "        'Revenue_Generated': np.random.randint(50000, 1000000, n_samples),\n",
        "        'Customer_Turnover_Rate': np.random.choice(['Low', 'Medium', 'High'], n_samples),\n",
        "        'Growth_Rate (%)': np.round(np.random.uniform(5, 25, n_samples), 2),\n",
        "        'Average_Monthly_Expenses': np.random.randint(10000, 500000, n_samples),\n",
        "        'Customer_Acquisition_Cost (₦)': np.random.randint(500, 5000, n_samples),\n",
        "        'Lifetime_Value_of_Customer (₦)': np.random.randint(5000, 100000, n_samples),\n",
        "        'Market_Size_Potential': np.random.randint(500, 50000, n_samples),\n",
        "        'Risk_Assessment': np.random.choice(['Low', 'Medium', 'High'], n_samples),\n",
        "        'Predicted_Revenue_Growth (%)': np.round(np.random.uniform(5, 30, n_samples), 2),\n",
        "        'Profitability_Score': np.round(np.random.uniform(50, 100, n_samples), 2),\n",
        "        'Churn_Rate (%)': np.round(np.random.uniform(5, 40, n_samples), 2),\n",
        "        'Recommendation_Actions': [generate_recommendation_actions() for _ in range(n_samples)]\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def improved_preprocessing(df):\n",
        "    # Enhanced feature engineering\n",
        "    df['Revenue_Growth_Rate'] = df.groupby('Month_Num')['Revenue_Generated'].pct_change()\n",
        "    df['Profit_Margin'] = df['Net_Profit'] / df['Revenue_Generated']\n",
        "    df['Operating_Margin'] = df['Operating_Income'] / df['Revenue_Generated']\n",
        "    df['Customer_Efficiency'] = df['Revenue_Generated'] / df['Market_Size_Potential']\n",
        "\n",
        "    # Add cyclical encoding for months\n",
        "    df['Month_Sin'] = np.sin(2 * np.pi * df['Month_Num'] / 12)\n",
        "    df['Month_Cos'] = np.cos(2 * np.pi * df['Month_Num'] / 12)\n",
        "\n",
        "    # Create interaction features\n",
        "    df['CAC_CLV_Ratio'] = df['Customer_Acquisition_Cost (₦)'] / df['Lifetime_Value_of_Customer (₦)']\n",
        "    df['Revenue_per_Expense'] = df['Revenue_Generated'] / df['Average_Monthly_Expenses']\n",
        "\n",
        "    # Add rolling statistics\n",
        "    df['Rolling_Avg_Revenue'] = df.groupby('Month_Num')['Revenue_Generated'].transform(\n",
        "        lambda x: x.rolling(3, min_periods=1).mean()\n",
        "    )\n",
        "    if 'Month_Num' not in df or df['Revenue_Generated'].isnull().any():\n",
        "      raise ValueError(\"Required columns are missing or contain NaNs.\")\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "class ImprovedBusinessDataProcessor:\n",
        "    def __init__(self):\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        self.label_encoder_turnover = LabelEncoder()\n",
        "        self.label_encoder_risk = LabelEncoder()\n",
        "        self.label_encoder_suggestions = LabelEncoder()\n",
        "\n",
        "    def process_data(self, df):\n",
        "        df = df.copy()\n",
        "\n",
        "        # Add month and quarter numbers\n",
        "        month_map = {month: idx for idx, month in enumerate(pd.date_range('2024-01-01', '2024-12-31', freq='M').strftime('%B'), 1)}\n",
        "        df['Month_Num'] = df['Monthly_Revenue_Turnover'].map(month_map)\n",
        "        df['Quarter_Num'] = ((df['Month_Num'] - 1) // 3) + 1\n",
        "\n",
        "        # Handle categorical variables\n",
        "        df['Customer_Turnover_Rate'] = self.label_encoder_turnover.fit_transform(df['Customer_Turnover_Rate'])\n",
        "        df['Risk_Assessment'] = self.label_encoder_risk.fit_transform(df['Risk_Assessment'])\n",
        "        df['Recommendation_Actions'] = self.label_encoder_suggestions.fit_transform(df['Recommendation_Actions'])\n",
        "\n",
        "        # Extract financial metrics from P&L Statement\n",
        "        df['Net_Profit'] = df['P&L_Statement'].apply(lambda x: float(x.split('Net Profit: ')[1]))\n",
        "        df['Operating_Income'] = df['P&L_Statement'].apply(lambda x: float(x.split('Operating Income: ')[1].split('\\n')[0]))\n",
        "\n",
        "        # Add derived features\n",
        "        df['Customer_Lifetime_Value_Ratio'] = df['Lifetime_Value_of_Customer (₦)'] / df['Customer_Acquisition_Cost (₦)']\n",
        "        df['Revenue_per_Market_Size'] = df['Revenue_Generated'] / df['Market_Size_Potential']\n",
        "        df['Expense_Ratio'] = df['Average_Monthly_Expenses'] / df['Revenue_Generated']\n",
        "\n",
        "        # Feature columns for scaling\n",
        "        feature_columns = [\n",
        "            'Revenue_Generated',\n",
        "            'Growth_Rate (%)',\n",
        "            'Average_Monthly_Expenses',\n",
        "            'Customer_Acquisition_Cost (₦)',\n",
        "            'Lifetime_Value_of_Customer (₦)',\n",
        "            'Market_Size_Potential',\n",
        "            'Customer_Lifetime_Value_Ratio',\n",
        "            'Revenue_per_Market_Size',\n",
        "            'Expense_Ratio',\n",
        "            'Net_Profit',\n",
        "            'Operating_Income'\n",
        "        ]\n",
        "\n",
        "        # Scale features\n",
        "        X = self.feature_scaler.fit_transform(df[feature_columns])\n",
        "\n",
        "        return (\n",
        "            X,\n",
        "            df['Predicted_Revenue_Growth (%)'].values,\n",
        "            df['Risk_Assessment'].values,\n",
        "            df['Profitability_Score'].values,\n",
        "            df['Churn_Rate (%)'].values / 100,  # Convert to proportion\n",
        "            df['Recommendation_Actions'].values,\n",
        "            df['Month_Num'].values,\n",
        "            df['Quarter_Num'].values\n",
        "        )\n",
        "\n",
        "def calculate_metrics(outputs, y_risk_batch, y_sug_batch):\n",
        "    risk_preds = torch.argmax(outputs['risk'], dim=1)\n",
        "    sug_preds = torch.argmax(outputs['suggestions'], dim=1)\n",
        "\n",
        "    risk_acc = (risk_preds == y_risk_batch).float().mean()\n",
        "    sug_acc = (sug_preds == y_sug_batch).float().mean()\n",
        "\n",
        "    return risk_acc.item(), sug_acc.item()\n",
        "\n",
        "def find_learning_rate(model, train_loader, device, start_lr=1e-7, end_lr=1, num_iterations=100):\n",
        "    \"\"\"\n",
        "    Implementation of the learning rate range test with better error handling.\n",
        "    \"\"\"\n",
        "    import copy\n",
        "    init_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    lrs = []\n",
        "    losses = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Create optimizer with initial learning rate\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr)\n",
        "\n",
        "    # Calculate multiplication factor\n",
        "    mult = (end_lr / start_lr) ** (1/num_iterations)\n",
        "\n",
        "    # Loss functions\n",
        "    regression_criterion = nn.MSELoss()\n",
        "    classification_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        if batch_idx >= num_iterations:\n",
        "            break\n",
        "\n",
        "        X_batch, y_revenue_batch, y_risk_batch, y_prof_batch, y_churn_batch, y_sug_batch, month_nums, quarter_nums = [\n",
        "            b.to(device) for b in batch\n",
        "        ]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            outputs = model(X_batch, month_nums, quarter_nums)\n",
        "\n",
        "            # Calculate losses\n",
        "            revenue_loss = regression_criterion(outputs['revenue'].squeeze(), y_revenue_batch)\n",
        "            risk_loss = classification_criterion(outputs['risk'], y_risk_batch)\n",
        "            prof_loss = regression_criterion(outputs['profitability'].squeeze(), y_prof_batch)\n",
        "            churn_loss = regression_criterion(outputs['churn'].squeeze(), y_churn_batch)\n",
        "            sug_loss = classification_criterion(outputs['suggestions'], y_sug_batch)\n",
        "\n",
        "            total_loss = revenue_loss + risk_loss + prof_loss + churn_loss + sug_loss\n",
        "\n",
        "            if not torch.isnan(total_loss) and not torch.isinf(total_loss):\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                lrs.append(optimizer.param_groups[0]['lr'])\n",
        "                losses.append(total_loss.item())\n",
        "\n",
        "                # Update learning rate\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= mult\n",
        "\n",
        "                # Early stopping if loss explodes\n",
        "                if total_loss.item() > 4 * best_loss:\n",
        "                    break\n",
        "                if total_loss.item() < best_loss:\n",
        "                    best_loss = total_loss.item()\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error during LR finding: {e}\")\n",
        "            break\n",
        "\n",
        "    # Restore initial model state\n",
        "    model.load_state_dict(init_state)\n",
        "\n",
        "    if len(losses) > 1:\n",
        "        # Find the point of steepest descent\n",
        "        loss_diff = np.diff(losses)\n",
        "        optimal_idx = np.argmin(loss_diff)\n",
        "        optimal_lr = lrs[optimal_idx] if optimal_idx < len(lrs) else lrs[-1]\n",
        "        return optimal_lr / 10, lrs, losses\n",
        "    else:\n",
        "        return 1e-4, lrs, losses\n",
        "\n",
        "def improved_train_model(model, train_loader, val_loader, device, num_epochs=100):\n",
        "    \"\"\"\n",
        "    Enhanced training function with better optimization and monitoring\n",
        "    Fixed loss handling to maintain computational graph for backpropagation\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=3e-4,\n",
        "        epochs=num_epochs, steps_per_epoch=len(train_loader)\n",
        "    )\n",
        "\n",
        "    # Loss functions with label smoothing for classification\n",
        "    regression_criterion = nn.HuberLoss(delta=1.0)  # More robust than MSE\n",
        "    classification_criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience = 20\n",
        "    patience_counter = 0\n",
        "\n",
        "    def calculate_metrics(outputs, targets):\n",
        "        revenue_batch, risk_batch, prof_batch, churn_batch, sug_batch = targets\n",
        "\n",
        "        # Calculate accuracies (detached for metrics)\n",
        "        risk_acc = (outputs['risk'].argmax(dim=1) == risk_batch).float().mean().item()\n",
        "        churn_acc = ((outputs['churn'].squeeze(1) > 0.5) == churn_batch).float().mean().item()\n",
        "        sug_acc = (outputs['suggestions'].argmax(dim=1) == sug_batch).float().mean().item()\n",
        "\n",
        "        # Calculate losses (keeping computational graph)\n",
        "        losses = {\n",
        "            'revenue': regression_criterion(outputs['revenue'].squeeze(1), revenue_batch),\n",
        "            'risk': classification_criterion(outputs['risk'], risk_batch),\n",
        "            'profitability': regression_criterion(outputs['profitability'].squeeze(1), prof_batch),\n",
        "            'churn': F.binary_cross_entropy(outputs['churn'].squeeze(1), churn_batch),\n",
        "            'suggestions': classification_criterion(outputs['suggestions'], sug_batch)\n",
        "        }\n",
        "\n",
        "        # Store loss values for metrics\n",
        "        loss_values = {k: v.item() for k, v in losses.items()}\n",
        "\n",
        "        accuracies = {\n",
        "            'risk': risk_acc,\n",
        "            'churn': churn_acc,\n",
        "            'suggestions': sug_acc\n",
        "        }\n",
        "\n",
        "        return losses, loss_values, accuracies\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_metrics = {\n",
        "            'risk_acc': [], 'churn_acc': [], 'suggestions_acc': [],\n",
        "            'revenue_loss': [], 'risk_loss': [], 'profitability_loss': [],\n",
        "            'churn_loss': [], 'suggestions_loss': []\n",
        "        }\n",
        "\n",
        "        # Training phase\n",
        "        for batch in train_loader:\n",
        "            X_batch, y_revenue_batch, y_risk_batch, y_prof_batch, \\\n",
        "            y_churn_batch, y_sug_batch, month_nums, quarter_nums = [b.to(device) for b in batch]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch, month_nums, quarter_nums)\n",
        "\n",
        "            losses, loss_values, accuracies = calculate_metrics(\n",
        "                outputs,\n",
        "                (y_revenue_batch, y_risk_batch, y_prof_batch, y_churn_batch, y_sug_batch)\n",
        "            )\n",
        "\n",
        "            # Sum losses while maintaining computational graph\n",
        "            total_loss = sum(losses.values())\n",
        "            total_loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Record metrics using detached values\n",
        "            train_losses.append(sum(loss_values.values()))\n",
        "            for k, v in loss_values.items():\n",
        "                train_metrics[f'{k}_loss'].append(v)\n",
        "            for k, v in accuracies.items():\n",
        "                train_metrics[f'{k}_acc'].append(v)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        val_metrics = {\n",
        "            'risk_acc': [], 'churn_acc': [], 'suggestions_acc': [],\n",
        "            'revenue_loss': [], 'risk_loss': [], 'profitability_loss': [],\n",
        "            'churn_loss': [], 'suggestions_loss': []\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                X_batch, y_revenue_batch, y_risk_batch, y_prof_batch, \\\n",
        "                y_churn_batch, y_sug_batch, month_nums, quarter_nums = [b.to(device) for b in batch]\n",
        "\n",
        "                outputs = model(X_batch, month_nums, quarter_nums)\n",
        "\n",
        "                _, loss_values, accuracies = calculate_metrics(\n",
        "                    outputs,\n",
        "                    (y_revenue_batch, y_risk_batch, y_prof_batch, y_churn_batch, y_sug_batch)\n",
        "                )\n",
        "\n",
        "                val_losses.append(sum(loss_values.values()))\n",
        "\n",
        "                # Record metrics\n",
        "                for k, v in loss_values.items():\n",
        "                    val_metrics[f'{k}_loss'].append(v)\n",
        "                for k, v in accuracies.items():\n",
        "                    val_metrics[f'{k}_acc'].append(v)\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "        # Print epoch metrics\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"Training Metrics:\")\n",
        "        print(f\"Total Loss: {avg_train_loss:.4f}\")\n",
        "        for k, v in train_metrics.items():\n",
        "            print(f\"{k}: {np.mean(v):.4f}\")\n",
        "\n",
        "        print(\"\\nValidation Metrics:\")\n",
        "        print(f\"Total Loss: {avg_val_loss:.4f}\")\n",
        "        for k, v in val_metrics.items():\n",
        "            print(f\"{k}: {np.mean(v):.4f}\")\n",
        "\n",
        "        # Model checkpointing\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model, {'best_val_loss': best_val_loss}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    set_seed(42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(\"Generating data...\")\n",
        "    df = generate_business_data(80000)\n",
        "\n",
        "    print(\"Processing data...\")\n",
        "    processor = ImprovedBusinessDataProcessor()\n",
        "    try:\n",
        "        X, y_revenue, y_risk, y_profitability, y_churn, y_suggestions, month_nums, quarter_nums = processor.process_data(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data processing: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"Splitting data...\")\n",
        "    X_train, X_val, y_revenue_train, y_revenue_val, \\\n",
        "    y_risk_train, y_risk_val, y_profitability_train, y_profitability_val, \\\n",
        "    y_churn_train, y_churn_val, y_suggestions_train, y_suggestions_val, \\\n",
        "    month_train, month_val, quarter_train, quarter_val = train_test_split(\n",
        "        X, y_revenue, y_risk, y_profitability, y_churn, y_suggestions, month_nums, quarter_nums, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Create datasets and loaders\n",
        "    train_dataset = BusinessDataset(\n",
        "        X_train, y_revenue_train, y_risk_train, y_profitability_train, y_churn_train,\n",
        "        y_suggestions_train, month_train, quarter_train\n",
        "    )\n",
        "    val_dataset = BusinessDataset(\n",
        "        X_val, y_revenue_val, y_risk_val, y_profitability_val, y_churn_val,\n",
        "        y_suggestions_val, month_val, quarter_val\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    print(\"Initializing model...\")\n",
        "    input_dim = X.shape[1]\n",
        "    num_risk_classes = len(np.unique(y_risk))\n",
        "    num_suggestion_classes = len(np.unique(y_suggestions))\n",
        "\n",
        "    model = EnhancedBusinessPredictor(\n",
        "        input_dim=input_dim,\n",
        "        num_risk_classes=num_risk_classes,\n",
        "        num_suggestion_classes=num_suggestion_classes\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"Finding optimal learning rate...\")\n",
        "    optimal_lr, lrs, losses = find_learning_rate(model, train_loader, device)\n",
        "    print(f\"Optimal learning rate found: {optimal_lr}\")\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    trained_model, metrics = improved_train_model(\n",
        "        model, train_loader, val_loader, device, num_epochs=100\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(\"Best validation loss:\", metrics['best_val_loss'])\n",
        "\n",
        "    # Save the model\n",
        "    # Save the model\n",
        "    model_save_path = \"improved_business_predictor.pth\"\n",
        "    torch.save({\n",
        "        'model_state_dict': trained_model.state_dict(),\n",
        "        'input_dim': input_dim,\n",
        "        'num_risk_classes': num_risk_classes,\n",
        "        'num_suggestion_classes': num_suggestion_classes\n",
        "    }, model_save_path)\n",
        "    print(f\"Model saved at: {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4CMmyBRiWG0",
        "outputId": "4de1f9a7-570b-405f-d1d1-e38c79e3c767"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlWjo2871yNl",
        "outputId": "a46e7496-b2b2-4d16-bb27-d8db3dc3848d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "class TimeSeriesBusinessPredictor:\n",
        "    def __init__(self, model_path: str, processor: ImprovedBusinessDataProcessor):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.processor = processor\n",
        "\n",
        "        # Load the saved model with weights_only=True for security\n",
        "        checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)\n",
        "\n",
        "        # Initialize the core prediction model\n",
        "        self.model = EnhancedBusinessPredictor(\n",
        "            input_dim=checkpoint['input_dim'],\n",
        "            num_risk_classes=checkpoint['num_risk_classes'],\n",
        "            num_suggestion_classes=checkpoint['num_suggestion_classes']\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load the trained weights\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        # Store the expected classes for validation\n",
        "        self.risk_classes = checkpoint.get('risk_classes', ['Low', 'Medium', 'High'])\n",
        "# Replace the original suggestion_classes with this expanded version\n",
        "        self.suggestion_classes = [\n",
        "            # Market Expansion & Growth\n",
        "            'Expand into new geographic markets through targeted marketing campaigns',\n",
        "            'Develop partnerships with complementary businesses for market penetration',\n",
        "            'Launch new product lines based on customer feedback and market research',\n",
        "            'Establish international presence through e-commerce platforms',\n",
        "            'Create franchise opportunities for rapid market expansion',\n",
        "\n",
        "            # Operational Optimization\n",
        "            'Implement automated inventory management system to reduce costs',\n",
        "            'Streamline supply chain through strategic supplier partnerships',\n",
        "            'Adopt lean manufacturing principles to minimize waste',\n",
        "            'Invest in employee training programs for improved productivity',\n",
        "            'Upgrade technology infrastructure for better operational efficiency',\n",
        "\n",
        "            # Customer Experience & Retention\n",
        "            'Develop personalized customer loyalty program with tiered benefits',\n",
        "            'Implement AI-powered customer service chatbot for 24/7 support',\n",
        "            'Create customer feedback loops with regular surveys and focus groups',\n",
        "            'Establish VIP customer program for high-value clients',\n",
        "            'Launch customer education programs about product features and benefits',\n",
        "\n",
        "            # Cost Management\n",
        "            'Negotiate bulk purchase agreements with suppliers for better rates',\n",
        "            'Optimize energy usage through smart building management systems',\n",
        "            'Implement zero-based budgeting for all departments',\n",
        "            'Outsource non-core business functions to reduce overhead',\n",
        "            'Invest in preventive maintenance to reduce long-term costs',\n",
        "\n",
        "            # Marketing & Brand Development\n",
        "            'Launch targeted social media advertising campaigns',\n",
        "            'Develop content marketing strategy with industry thought leadership',\n",
        "            'Create referral program with incentives for existing customers',\n",
        "            'Implement influencer marketing program in key markets',\n",
        "            'Enhance brand visibility through community engagement events',\n",
        "\n",
        "            # Digital Transformation\n",
        "            'Develop mobile app for improved customer engagement',\n",
        "            'Implement cloud-based solutions for remote work capability',\n",
        "            'Create digital payment options for customer convenience',\n",
        "            'Establish omnichannel presence for seamless customer experience',\n",
        "            'Implement data analytics for better decision-making',\n",
        "\n",
        "            # Product Innovation\n",
        "            'Conduct research and development for product improvements',\n",
        "            'Create sustainable/eco-friendly product alternatives',\n",
        "            'Develop subscription-based service models',\n",
        "            'Launch premium product line for high-end market segment',\n",
        "            'Create bundled product offerings for increased value',\n",
        "\n",
        "            # Financial Management\n",
        "            'Implement dynamic pricing strategy based on market demand',\n",
        "            'Develop alternative revenue streams through complementary services',\n",
        "            'Optimize working capital through improved inventory management',\n",
        "            'Establish strategic partnerships for shared resource utilization',\n",
        "            'Create financial forecasting models for better planning',\n",
        "\n",
        "            # Human Resources\n",
        "            'Implement performance-based incentive programs',\n",
        "            'Develop career advancement programs for employee retention',\n",
        "            'Create flexible work arrangements for improved work-life balance',\n",
        "            'Establish mentorship programs for knowledge transfer',\n",
        "            'Implement employee wellness programs for improved productivity',\n",
        "\n",
        "            # Risk Management\n",
        "            'Develop business continuity plans for various scenarios',\n",
        "            'Implement cybersecurity measures for data protection',\n",
        "            'Create disaster recovery protocols for critical systems',\n",
        "            'Establish quality control processes for consistent delivery',\n",
        "            'Develop compliance monitoring systems for regulatory requirements'\n",
        "        ]\n",
        "\n",
        "        # Initialize prediction history\n",
        "        self.prediction_history = []\n",
        "\n",
        "    def _validate_prediction(self, pred_idx: int, valid_classes: List[str]) -> str:\n",
        "        \"\"\"Validate prediction index against known classes\"\"\"\n",
        "        if 0 <= pred_idx < len(valid_classes):\n",
        "            return valid_classes[pred_idx]\n",
        "        return \"Unknown\"\n",
        "\n",
        "    def _calculate_stability(self) -> float:\n",
        "        \"\"\"Calculate the stability of predictions over time.\"\"\"\n",
        "        if len(self.prediction_history) < 2:\n",
        "            return 1.0  # If there's only one prediction, it's stable by default\n",
        "\n",
        "        # Calculate percentage change in revenue and profitability from the previous prediction\n",
        "        prev_pred = self.prediction_history[-2]\n",
        "        curr_pred = self.prediction_history[-1]\n",
        "\n",
        "        revenue_change = (curr_pred['revenue'] - prev_pred['revenue']) / prev_pred['revenue'] if prev_pred['revenue'] != 0 else 0\n",
        "        profitability_change = (curr_pred['profitability'] - prev_pred['profitability']) / prev_pred['profitability'] if prev_pred['profitability'] != 0 else 0\n",
        "\n",
        "        # Stability score based on the magnitude of changes (higher score is more stable)\n",
        "        stability_score = 1 - (abs(revenue_change) + abs(profitability_change)) / 2\n",
        "\n",
        "        return max(0, stability_score)  # Ensure stability is between 0 and 1\n",
        "\n",
        "\n",
        "    def _calculate_confidence_metrics(self, outputs: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        \"\"\"Calculate confidence scores for various predictions\"\"\"\n",
        "        risk_probs = F.softmax(outputs['risk'], dim=1)\n",
        "        action_probs = F.softmax(outputs['suggestions'], dim=1)\n",
        "\n",
        "        # Use robust standard deviation calculation\n",
        "        revenue_std = torch.std(outputs['revenue'], unbiased=False).item() if outputs['revenue'].numel() > 1 else 0.0\n",
        "\n",
        "        return {\n",
        "            'risk_confidence': risk_probs.max().item(),\n",
        "            'action_confidence': action_probs.max().item(),\n",
        "            'revenue_uncertainty': revenue_std,\n",
        "            'prediction_stability': self._calculate_stability()\n",
        "        }\n",
        "\n",
        "\n",
        "    def _update_features(self, current_features: torch.Tensor, current_metrics: Dict[str, float], month: int) -> torch.Tensor:\n",
        "        \"\"\"Update features with new monthly metrics and add seasonal variation.\"\"\"\n",
        "        # Extract relevant features from current_metrics\n",
        "        base_features = [\n",
        "            current_metrics['revenue'],\n",
        "            current_metrics['growth_rate'],\n",
        "            current_metrics['expenses'],\n",
        "            current_metrics['cac'],\n",
        "            current_metrics['ltv'],\n",
        "            current_metrics['market_size'],\n",
        "            current_metrics['ltv'] / current_metrics['cac'],\n",
        "            current_metrics['revenue'] / current_metrics['market_size'],\n",
        "            current_metrics['expenses'] / current_metrics['revenue'],\n",
        "            current_metrics['revenue'] * np.random.normal(0.25, 0.03),  # Variable profit margin\n",
        "            current_metrics['revenue'] * np.random.normal(0.2, 0.02),   # Variable operating income\n",
        "        ]\n",
        "\n",
        "        # Add seasonal variation based on month\n",
        "        seasonal_factor = 1.0 + 0.1 * np.sin(2 * np.pi * month / 12)  # Creates yearly cycle\n",
        "        market_noise = np.random.normal(1.0, 0.05)  # Random market fluctuations\n",
        "\n",
        "        # Apply seasonal and random variations\n",
        "        new_features = [f * seasonal_factor * market_noise for f in base_features]\n",
        "\n",
        "        # Scale new features using the same scaler used for training data\n",
        "        new_features_scaled = self.processor.feature_scaler.transform([new_features])\n",
        "\n",
        "        # Add random noise to prevent perfect convergence\n",
        "        noise = torch.randn_like(current_features) * 0.01\n",
        "\n",
        "        # Replace relevant feature values in the current_features tensor\n",
        "        X_updated = current_features.clone()\n",
        "        X_updated[0, :len(new_features_scaled[0])] = torch.tensor(new_features_scaled[0]).to(self.device).type(X_updated.dtype)\n",
        "        X_updated += noise\n",
        "\n",
        "        return X_updated\n",
        "\n",
        "    def _calculate_risk_level(self,\n",
        "                            outputs: Dict[str, torch.Tensor],\n",
        "                            confidence: float,\n",
        "                            threshold: float,\n",
        "                            current_metrics: Dict[str, float]) -> Tuple[str, float]:\n",
        "        \"\"\"Calculate risk level using multiple factors\"\"\"\n",
        "        risk_probs = F.softmax(outputs['risk'], dim=1)\n",
        "        max_prob = risk_probs.max().item()\n",
        "\n",
        "        # Calculate additional risk factors\n",
        "        growth_risk = 1.0 if current_metrics['growth_rate'] < 0 else 0.0\n",
        "        churn_risk = outputs['churn'].item()\n",
        "        profitability = outputs['profitability'].item()\n",
        "\n",
        "        # Combine risk factors\n",
        "        risk_score = (\n",
        "            0.4 * max_prob +\n",
        "            0.2 * growth_risk +\n",
        "            0.2 * churn_risk +\n",
        "            0.2 * (1 - profitability/100)\n",
        "        )\n",
        "\n",
        "        # Determine risk level\n",
        "        if confidence < threshold:\n",
        "            return \"Uncertain\", risk_score\n",
        "        elif risk_score < 0.3:\n",
        "            return \"Low\", risk_score\n",
        "        elif risk_score < 0.6:\n",
        "            return \"Medium\", risk_score\n",
        "        else:\n",
        "            return \"High\", risk_score\n",
        "\n",
        "    def _get_unique_recommendations(self, top_actions, num_recommendations, recent_actions):\n",
        "        \"\"\"Generate unique recommendations avoiding recent duplicates.\"\"\"\n",
        "        unique_recommendations = []\n",
        "        unique_confidences = []\n",
        "\n",
        "        for idx, (action_idx, confidence) in enumerate(zip(top_actions.indices[0], top_actions.values[0])):\n",
        "            action_label = self._validate_prediction(action_idx.item(), self.suggestion_classes)\n",
        "\n",
        "            # Avoid actions already recommended recently\n",
        "            if action_label not in recent_actions and action_label not in unique_recommendations:\n",
        "                unique_recommendations.append(action_label)\n",
        "                unique_confidences.append(confidence.item())\n",
        "\n",
        "            # Break loop if we have enough unique recommendations\n",
        "            if len(unique_recommendations) >= num_recommendations:\n",
        "                break\n",
        "\n",
        "            # If not enough unique recommendations, allow some repeats with reduced confidence\n",
        "            while len(unique_recommendations) < num_recommendations:\n",
        "                fallback_idx = np.random.choice(top_actions.indices[0].cpu().numpy())\n",
        "                fallback_label = self._validate_prediction(fallback_idx.item(), self.suggestion_classes)\n",
        "                if fallback_label not in unique_recommendations:\n",
        "                    unique_recommendations.append(fallback_label)\n",
        "                    unique_confidences.append(0.5)  # Assign lower confidence for fallback actions\n",
        "\n",
        "            return unique_recommendations, unique_confidences\n",
        "\n",
        "    def predict_monthly_metrics(\n",
        "        self,\n",
        "        initial_data: pd.DataFrame,\n",
        "        num_months: int = 12,\n",
        "        confidence_threshold: float = 0.7,\n",
        "        num_recommendations: int = 3  # Number of recommendations to return\n",
        "    ) -> Tuple[Dict[str, List], List[Dict[str, float]]]:\n",
        "        \"\"\"Make predictions with multiple recommendations per period\"\"\"\n",
        "        predictions = {\n",
        "            'dates': [],\n",
        "            'revenue': [],\n",
        "            'revenue_growth': [],\n",
        "            'risk_levels': [],\n",
        "            'risk_scores': [],\n",
        "            'profitability': [],\n",
        "            'churn_probability': [],\n",
        "            'recommended_actions': [],  # Now will contain lists of recommendations\n",
        "            'action_confidences': [],   # Store confidence scores for each recommendation\n",
        "        }\n",
        "\n",
        "        confidence_metrics = []\n",
        "        start_date = pd.Timestamp.now().replace(day=1)\n",
        "        current_data = initial_data.copy()\n",
        "\n",
        "        # Process initial data\n",
        "        features, _, _, _, _, _, month_nums, quarter_nums = self.processor.process_data(current_data)\n",
        "        X = torch.FloatTensor(features).to(self.device)\n",
        "\n",
        "        # Initialize current_metrics before the loop.\n",
        "        current_metrics = {\n",
        "            'revenue': current_data['Revenue_Generated'].values[0],\n",
        "            'growth_rate': current_data['Growth_Rate (%)'].values[0],\n",
        "            'expenses': current_data['Average_Monthly_Expenses'].values[0],\n",
        "            'cac': current_data['Customer_Acquisition_Cost (₦)'].values[0],\n",
        "            'ltv': current_data['Lifetime_Value_of_Customer (₦)'].values[0],\n",
        "            'market_size': current_data['Market_Size_Potential'].values[0]\n",
        "        }\n",
        "\n",
        "        for month in range(num_months):\n",
        "            current_date = start_date + pd.DateOffset(months=month)\n",
        "            current_month = current_date.month\n",
        "            current_quarter = (current_month - 1) // 3 + 1\n",
        "\n",
        "            # Prepare temporal features\n",
        "            month_tensor = torch.LongTensor([current_month]).to(self.device)\n",
        "            quarter_tensor = torch.LongTensor([current_quarter]).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Get model outputs\n",
        "                outputs = self.model(X, month_tensor, quarter_tensor)\n",
        "                # print(f\"outputs from model{outputs}\")\n",
        "\n",
        "                # Calculate confidence metrics\n",
        "                confidence = self._calculate_confidence_metrics(outputs)\n",
        "\n",
        "                # Compute action probabilities\n",
        "                action_probs = F.softmax(outputs['suggestions'], dim=1)\n",
        "\n",
        "                # Dynamically adjust k for torch.topk\n",
        "                num_actions = action_probs.size(1)\n",
        "                k = min(10, num_actions)\n",
        "\n",
        "                # Generate recommendations with diversity constraints\n",
        "                top_actions = torch.topk(action_probs, k, dim=1)  # Adjusted k\n",
        "                recent_actions = predictions['recommended_actions'][-3:] if len(predictions['recommended_actions']) > 3 else []\n",
        "                month_recommendations, month_confidences = self._get_unique_recommendations(\n",
        "                    top_actions, num_recommendations, recent_actions\n",
        "                )\n",
        "\n",
        "            # Calculate revenue with seasonal variation\n",
        "            base_growth = outputs['revenue'].item()\n",
        "            seasonal_factor = 1.0 + 0.1 * np.sin(2 * np.pi * current_month / 12)\n",
        "            market_noise = np.random.normal(1.0, 0.02)\n",
        "            revenue_growth = base_growth * seasonal_factor * market_noise\n",
        "\n",
        "            current_revenue = (\n",
        "                current_data['Revenue_Generated'].values[0] if month == 0\n",
        "                else predictions['revenue'][-1] * (1 + revenue_growth / 100)\n",
        "            )\n",
        "\n",
        "            # Calculate risk with improved assessment\n",
        "            current_metrics = {\n",
        "                'revenue': current_revenue,\n",
        "                'growth_rate': revenue_growth,\n",
        "                'expenses': current_data['Average_Monthly_Expenses'].values[0],\n",
        "                'cac': current_data['Customer_Acquisition_Cost (₦)'].values[0],\n",
        "                'ltv': current_data['Lifetime_Value_of_Customer (₦)'].values[0],\n",
        "                'market_size': current_data['Market_Size_Potential'].values[0]\n",
        "            }\n",
        "\n",
        "            risk_label, risk_score = self._calculate_risk_level(\n",
        "                outputs,\n",
        "                confidence['risk_confidence'],\n",
        "                confidence_threshold,\n",
        "                current_metrics\n",
        "            )\n",
        "\n",
        "            # Store predictions with added variability\n",
        "            predictions['dates'].append(current_date)\n",
        "            predictions['revenue'].append(current_revenue)\n",
        "            predictions['revenue_growth'].append(revenue_growth)\n",
        "            predictions['risk_levels'].append(risk_label)\n",
        "            predictions['risk_scores'].append(risk_score)\n",
        "            predictions['profitability'].append(outputs['profitability'].item() * np.random.normal(1.0, 0.05))\n",
        "            predictions['churn_probability'].append(outputs['churn'].item() * np.random.normal(1.0, 0.03))\n",
        "            predictions['recommended_actions'].append(month_recommendations)\n",
        "            predictions['action_confidences'].append(month_confidences)\n",
        "\n",
        "            confidence_metrics.append(confidence)\n",
        "\n",
        "            # Update features for next prediction\n",
        "            X = self._update_features(X, current_metrics, current_month)\n",
        "\n",
        "        # Return predictions after processing all months\n",
        "        return predictions, confidence_metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create sample current metrics\n",
        "    current_metrics = pd.DataFrame({\n",
        "        'P&L_Statement': ['Revenue: 1000000\\nCOGS: 600000\\nGross Profit: 400000\\n' +\n",
        "                         'Operating Expenses: 200000\\nOperating Income: 200000\\n' +\n",
        "                         'Other Income: 50000\\nNet Profit: 250000'],\n",
        "        'Balance_Sheet': ['Current Assets: 800000\\nFixed Assets: 2000000\\n' +\n",
        "                         'Total Assets: 2800000\\nCurrent Liabilities: 300000\\n' +\n",
        "                         'Long-term Liabilities: 500000\\nTotal Liabilities: 800000\\n' +\n",
        "                         'Equity: 2000000'],\n",
        "        'Monthly_Revenue_Turnover': ['January'],\n",
        "        'Revenue_Generated': [1000000],\n",
        "        'Customer_Turnover_Rate': ['Low'],\n",
        "        'Growth_Rate (%)': [15.0],\n",
        "        'Average_Monthly_Expenses': [200000],\n",
        "        'Customer_Acquisition_Cost (₦)': [2000],\n",
        "        'Lifetime_Value_of_Customer (₦)': [20000],\n",
        "        'Market_Size_Potential': [10000],\n",
        "        'Risk_Assessment': ['Low'],\n",
        "        'Predicted_Revenue_Growth (%)': [20.0],\n",
        "        'Profitability_Score': [75.0],\n",
        "        'Churn_Rate (%)': [10.0],\n",
        "        'Recommendation_Actions': ['Expand market reach']\n",
        "    })\n",
        "\n",
        "    # Initialize predictor\n",
        "    processor = ImprovedBusinessDataProcessor()\n",
        "    predictor = TimeSeriesBusinessPredictor('improved_business_predictor.pth', processor)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions, confidence_metrics = predictor.predict_monthly_metrics(\n",
        "        initial_data=current_metrics,\n",
        "        num_months=12,\n",
        "        confidence_threshold=0.7\n",
        "    )\n",
        "\n",
        "    # print(predictions)\n",
        "\n",
        "    # Print predictions\n",
        "    print(\"\\nPredictions for the next 12 months:\")\n",
        "    for i in range(len(predictions['dates'])):\n",
        "        print(f\"\\nMonth {predictions['dates'][i].strftime('%Y-%m')}:\")\n",
        "        print(f\"Revenue Growth: {predictions['revenue_growth'][i]:.2f}%\")\n",
        "        print(f\"Predicted Revenue: ₦{predictions['revenue'][i]:,.2f}\")\n",
        "        print(f\"Risk Level: {predictions['risk_levels'][i]}\")\n",
        "        print(f\"Profitability Score: {predictions['profitability'][i]:.2f}\")\n",
        "        print(f\"Churn Probability: {predictions['churn_probability'][i]*100:.2f}%\")\n",
        "        print(f\"Recommended Action: {predictions['recommended_actions'][i]}\")\n",
        "\n",
        "    # Visualize predictions\n",
        "    # predictor.visualize_predictions(predictions, confidence_metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
